{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Vision Transformer (ViT) on CIFAR-10 - **IMPROVED VERSION**\n",
        "\n",
        "**Target: 88-92% Test Accuracy**\n",
        "\n",
        "Based on: *An Image is Worth 16x16 Words: Transformers for Image Recognition at Scale*\n",
        "\n",
        "---\n",
        "\n",
        "## Key Improvements Over Base Version\n",
        "\n",
        "1. **Stronger Data Augmentation**: AutoAugment + Cutout\n",
        "2. **Label Smoothing**: Reduces overfitting\n",
        "3. **Stochastic Depth**: Dropout for transformer blocks\n",
        "4. **Longer Training**: 200 epochs instead of 100\n",
        "5. **Better Regularization**: Higher dropout + weight decay\n",
        "6. **Gradient Accumulation**: Effective batch size 256\n",
        "\n",
        "---\n",
        "\n",
        "## Expected Results\n",
        "\n",
        "| Configuration | Test Accuracy | Training Time |\n",
        "|---------------|---------------|---------------|\n",
        "| Previous | 83-84% | ~1 hour |\n",
        "| **This Version** | **88-92%** | **~3 hours** |\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Imports\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torchvision.datasets import CIFAR10\n",
        "\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.auto import tqdm\n",
        "import time\n",
        "import math\n",
        "import random\n",
        "\n",
        "print(f\"PyTorch: {torch.__version__}\")\n",
        "print(f\"CUDA: {torch.cuda.is_available()}\")\n",
        "\n",
        "# Seeds\n",
        "def set_seed(seed=42):\n",
        "    torch.manual_seed(seed)\n",
        "    np.random.seed(seed)\n",
        "    random.seed(seed)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(seed)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "set_seed(42)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPROVED Configuration\n",
        "class Config:\n",
        "    # Model\n",
        "    img_size = 32\n",
        "    patch_size = 4\n",
        "    in_channels = 3\n",
        "    num_classes = 10\n",
        "    embed_dim = 384  # Increased from 256\n",
        "    depth = 7        # Increased from 6\n",
        "    num_heads = 8\n",
        "    mlp_ratio = 3.0  # Increased from 2.0\n",
        "    dropout = 0.2    # Increased from 0.1\n",
        "    drop_path = 0.1  # Stochastic depth\n",
        "    \n",
        "    # Training\n",
        "    batch_size = 128\n",
        "    accumulation_steps = 2  # Effective batch size = 256\n",
        "    num_epochs = 200        # Increased from 100\n",
        "    learning_rate = 5e-4    # Slightly higher\n",
        "    weight_decay = 0.05\n",
        "    warmup_epochs = 10\n",
        "    label_smoothing = 0.1   # NEW\n",
        "    \n",
        "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "\n",
        "config = Config()\n",
        "print(f\"Training on: {config.device}\")\n",
        "print(f\"Effective batch size: {config.batch_size * config.accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Stochastic Depth (Drop Path)\n",
        "def drop_path(x, drop_prob=0., training=False):\n",
        "    if drop_prob == 0. or not training:\n",
        "        return x\n",
        "    keep_prob = 1 - drop_prob\n",
        "    shape = (x.shape[0],) + (1,) * (x.ndim - 1)\n",
        "    random_tensor = keep_prob + torch.rand(shape, dtype=x.dtype, device=x.device)\n",
        "    random_tensor.floor_()\n",
        "    output = x.div(keep_prob) * random_tensor\n",
        "    return output\n",
        "\n",
        "class DropPath(nn.Module):\n",
        "    def __init__(self, drop_prob=None):\n",
        "        super().__init__()\n",
        "        self.drop_prob = drop_prob\n",
        "\n",
        "    def forward(self, x):\n",
        "        return drop_path(x, self.drop_prob, self.training)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Patch Embedding\n",
        "class PatchEmbedding(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_channels=3, embed_dim=384):\n",
        "        super().__init__()\n",
        "        self.num_patches = (img_size // patch_size) ** 2\n",
        "        self.projection = nn.Conv2d(in_channels, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = self.projection(x)\n",
        "        x = x.flatten(2).transpose(1, 2)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Attention\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads=8, qkv_bias=True, attn_drop=0., proj_drop=0.):\n",
        "        super().__init__()\n",
        "        self.num_heads = num_heads\n",
        "        head_dim = dim // num_heads\n",
        "        self.scale = head_dim ** -0.5\n",
        "\n",
        "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
        "        self.attn_drop = nn.Dropout(attn_drop)\n",
        "        self.proj = nn.Linear(dim, dim)\n",
        "        self.proj_drop = nn.Dropout(proj_drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B, N, C = x.shape\n",
        "        qkv = self.qkv(x).reshape(B, N, 3, self.num_heads, C // self.num_heads).permute(2, 0, 3, 1, 4)\n",
        "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
        "        attn = attn.softmax(dim=-1)\n",
        "        attn = self.attn_drop(attn)\n",
        "\n",
        "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
        "        x = self.proj(x)\n",
        "        x = self.proj_drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# MLP\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, in_features, hidden_features, drop=0.):\n",
        "        super().__init__()\n",
        "        self.fc1 = nn.Linear(in_features, hidden_features)\n",
        "        self.act = nn.GELU()\n",
        "        self.fc2 = nn.Linear(hidden_features, in_features)\n",
        "        self.drop = nn.Dropout(drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.fc1(x)\n",
        "        x = self.act(x)\n",
        "        x = self.drop(x)\n",
        "        x = self.fc2(x)\n",
        "        x = self.drop(x)\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Transformer Block with Drop Path\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, mlp_ratio=4., drop=0., attn_drop=0., drop_path=0.):\n",
        "        super().__init__()\n",
        "        self.norm1 = nn.LayerNorm(dim)\n",
        "        self.attn = Attention(dim, num_heads=num_heads, qkv_bias=True, attn_drop=attn_drop, proj_drop=drop)\n",
        "        self.drop_path = DropPath(drop_path) if drop_path > 0. else nn.Identity()\n",
        "        self.norm2 = nn.LayerNorm(dim)\n",
        "        mlp_hidden_dim = int(dim * mlp_ratio)\n",
        "        self.mlp = MLP(in_features=dim, hidden_features=mlp_hidden_dim, drop=drop)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.drop_path(self.attn(self.norm1(x)))\n",
        "        x = x + self.drop_path(self.mlp(self.norm2(x)))\n",
        "        return x"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Vision Transformer\n",
        "class VisionTransformer(nn.Module):\n",
        "    def __init__(self, img_size=32, patch_size=4, in_chans=3, num_classes=10,\n",
        "                 embed_dim=384, depth=7, num_heads=8, mlp_ratio=3., drop_rate=0.2, drop_path_rate=0.1):\n",
        "        super().__init__()\n",
        "        \n",
        "        self.patch_embed = PatchEmbedding(img_size, patch_size, in_chans, embed_dim)\n",
        "        num_patches = self.patch_embed.num_patches\n",
        "\n",
        "        self.cls_token = nn.Parameter(torch.zeros(1, 1, embed_dim))\n",
        "        self.pos_embed = nn.Parameter(torch.zeros(1, num_patches + 1, embed_dim))\n",
        "        self.pos_drop = nn.Dropout(p=drop_rate)\n",
        "\n",
        "        # Stochastic depth decay rule\n",
        "        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]\n",
        "        self.blocks = nn.ModuleList([\n",
        "            TransformerBlock(embed_dim, num_heads, mlp_ratio, drop_rate, drop_rate, dpr[i])\n",
        "            for i in range(depth)\n",
        "        ])\n",
        "        \n",
        "        self.norm = nn.LayerNorm(embed_dim)\n",
        "        self.head = nn.Linear(embed_dim, num_classes)\n",
        "\n",
        "        # Init\n",
        "        nn.init.trunc_normal_(self.pos_embed, std=0.02)\n",
        "        nn.init.trunc_normal_(self.cls_token, std=0.02)\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, nn.Linear):\n",
        "            nn.init.trunc_normal_(m.weight, std=0.02)\n",
        "            if m.bias is not None:\n",
        "                nn.init.zeros_(m.bias)\n",
        "        elif isinstance(m, nn.LayerNorm):\n",
        "            nn.init.zeros_(m.bias)\n",
        "            nn.init.ones_(m.weight)\n",
        "\n",
        "    def forward(self, x):\n",
        "        B = x.shape[0]\n",
        "        x = self.patch_embed(x)\n",
        "        cls_tokens = self.cls_token.expand(B, -1, -1)\n",
        "        x = torch.cat((cls_tokens, x), dim=1)\n",
        "        x = x + self.pos_embed\n",
        "        x = self.pos_drop(x)\n",
        "\n",
        "        for blk in self.blocks:\n",
        "            x = blk(x)\n",
        "\n",
        "        x = self.norm(x)\n",
        "        return self.head(x[:, 0])\n",
        "\n",
        "print(\"\u2713 Improved ViT architecture defined\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# IMPROVED Data Augmentation\n",
        "def get_data_loaders(config):\n",
        "    mean = (0.4914, 0.4822, 0.4465)\n",
        "    std = (0.2471, 0.2435, 0.2616)\n",
        "\n",
        "    # Strong augmentation\n",
        "    train_transform = transforms.Compose([\n",
        "        transforms.RandomCrop(32, padding=4),\n",
        "        transforms.RandomHorizontalFlip(),\n",
        "        transforms.AutoAugment(policy=transforms.AutoAugmentPolicy.CIFAR10),  # NEW\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std),\n",
        "        transforms.RandomErasing(p=0.25),  # Cutout\n",
        "    ])\n",
        "\n",
        "    test_transform = transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize(mean, std)\n",
        "    ])\n",
        "\n",
        "    train_dataset = CIFAR10(root='./data', train=True, download=True, transform=train_transform)\n",
        "    test_dataset = CIFAR10(root='./data', train=False, download=True, transform=test_transform)\n",
        "\n",
        "    train_loader = DataLoader(train_dataset, batch_size=config.batch_size, shuffle=True, \n",
        "                              num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "    test_loader = DataLoader(test_dataset, batch_size=config.batch_size, shuffle=False, \n",
        "                             num_workers=2, pin_memory=True, persistent_workers=True)\n",
        "\n",
        "    return train_loader, test_loader\n",
        "\n",
        "train_loader, test_loader = get_data_loaders(config)\n",
        "print(f\"\u2713 Data with strong augmentation: {len(train_loader.dataset)} train, {len(test_loader.dataset)} test\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Create model\n",
        "model = VisionTransformer(\n",
        "    img_size=config.img_size,\n",
        "    patch_size=config.patch_size,\n",
        "    in_chans=config.in_channels,\n",
        "    num_classes=config.num_classes,\n",
        "    embed_dim=config.embed_dim,\n",
        "    depth=config.depth,\n",
        "    num_heads=config.num_heads,\n",
        "    mlp_ratio=config.mlp_ratio,\n",
        "    drop_rate=config.dropout,\n",
        "    drop_path_rate=config.drop_path\n",
        ").to(config.device)\n",
        "\n",
        "n_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\u2713 Model: {n_params:,} parameters\")\n",
        "print(f\"\u2713 Embed dim: {config.embed_dim}, Depth: {config.depth}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Label Smoothing Loss\n",
        "class LabelSmoothingCrossEntropy(nn.Module):\n",
        "    def __init__(self, smoothing=0.1):\n",
        "        super().__init__()\n",
        "        self.smoothing = smoothing\n",
        "\n",
        "    def forward(self, pred, target):\n",
        "        n_class = pred.size(1)\n",
        "        one_hot = torch.zeros_like(pred).scatter(1, target.view(-1, 1), 1)\n",
        "        one_hot = one_hot * (1 - self.smoothing) + (1 - one_hot) * self.smoothing / (n_class - 1)\n",
        "        log_prob = F.log_softmax(pred, dim=1)\n",
        "        return -(one_hot * log_prob).sum(dim=1).mean()\n",
        "\n",
        "criterion = LabelSmoothingCrossEntropy(smoothing=config.label_smoothing)\n",
        "print(f\"\u2713 Using label smoothing: {config.label_smoothing}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Optimizer & Scheduler\n",
        "def get_lr_scheduler(optimizer, warmup_epochs, total_epochs, steps_per_epoch):\n",
        "    warmup_steps = warmup_epochs * steps_per_epoch\n",
        "    total_steps = total_epochs * steps_per_epoch\n",
        "    \n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return step / warmup_steps\n",
        "        progress = (step - warmup_steps) / (total_steps - warmup_steps)\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    \n",
        "    return optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "optimizer = optim.AdamW(model.parameters(), lr=config.learning_rate, weight_decay=config.weight_decay)\n",
        "scheduler = get_lr_scheduler(optimizer, config.warmup_epochs, config.num_epochs, len(train_loader))\n",
        "print(\"\u2713 Optimizer: AdamW with cosine annealing\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Training with Gradient Accumulation\n",
        "def train_epoch(model, loader, criterion, optimizer, scheduler, device, accumulation_steps):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    optimizer.zero_grad()\n",
        "    pbar = tqdm(loader, desc='Training')\n",
        "    \n",
        "    for i, (inputs, targets) in enumerate(pbar):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        \n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        loss = loss / accumulation_steps\n",
        "        loss.backward()\n",
        "        \n",
        "        if (i + 1) % accumulation_steps == 0:\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "        \n",
        "        total_loss += loss.item() * accumulation_steps\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "        \n",
        "        pbar.set_postfix({\n",
        "            'loss': f'{total_loss/(pbar.n+1):.3f}',\n",
        "            'acc': f'{100.*correct/total:.2f}%',\n",
        "            'lr': f'{scheduler.get_last_lr()[0]:.2e}'\n",
        "        })\n",
        "    \n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "@torch.no_grad()\n",
        "def evaluate(model, loader, criterion, device):\n",
        "    model.eval()\n",
        "    total_loss = 0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    \n",
        "    for inputs, targets in tqdm(loader, desc='Evaluating'):\n",
        "        inputs, targets = inputs.to(device), targets.to(device)\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, targets)\n",
        "        \n",
        "        total_loss += loss.item()\n",
        "        _, predicted = outputs.max(1)\n",
        "        total += targets.size(0)\n",
        "        correct += predicted.eq(targets).sum().item()\n",
        "    \n",
        "    return total_loss / len(loader), 100. * correct / total\n",
        "\n",
        "print(\"\u2713 Training functions ready\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Main training loop\n",
        "print(\"=\"*70)\n",
        "print(\"Starting IMPROVED training (target: 88-92% accuracy)\")\n",
        "print(\"=\"*70)\n",
        "\n",
        "best_acc = 0\n",
        "train_losses, test_losses = [], []\n",
        "train_accs, test_accs = [], []\n",
        "start_time = time.time()\n",
        "\n",
        "for epoch in range(config.num_epochs):\n",
        "    print(f\"\\nEpoch {epoch+1}/{config.num_epochs}\")\n",
        "    print(\"-\"*70)\n",
        "    \n",
        "    train_loss, train_acc = train_epoch(model, train_loader, criterion, optimizer, \n",
        "                                        scheduler, config.device, config.accumulation_steps)\n",
        "    test_loss, test_acc = evaluate(model, test_loader, criterion, config.device)\n",
        "    \n",
        "    train_losses.append(train_loss)\n",
        "    test_losses.append(test_loss)\n",
        "    train_accs.append(train_acc)\n",
        "    test_accs.append(test_acc)\n",
        "    \n",
        "    print(f\"Train: Loss={train_loss:.4f}, Acc={train_acc:.2f}%\")\n",
        "    print(f\"Test:  Loss={test_loss:.4f}, Acc={test_acc:.2f}%\")\n",
        "    \n",
        "    if test_acc > best_acc:\n",
        "        best_acc = test_acc\n",
        "        torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'test_acc': test_acc,\n",
        "            'config': vars(config)\n",
        "        }, 'best_vit_improved.pth')\n",
        "        print(f\"\u2713 Best: {best_acc:.2f}%\")\n",
        "\n",
        "elapsed = time.time() - start_time\n",
        "print(f\"\\n{'='*70}\")\n",
        "print(f\"Training complete in {elapsed/3600:.2f} hours\")\n",
        "print(f\"Best test accuracy: {best_acc:.2f}%\")\n",
        "print(f\"{'='*70}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Plot results\n",
        "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "epochs = range(1, len(train_losses) + 1)\n",
        "\n",
        "ax1.plot(epochs, train_losses, 'b-', label='Train Loss', linewidth=2)\n",
        "ax1.plot(epochs, test_losses, 'r-', label='Test Loss', linewidth=2)\n",
        "ax1.set_xlabel('Epoch', fontsize=12)\n",
        "ax1.set_ylabel('Loss', fontsize=12)\n",
        "ax1.set_title('Training and Test Loss', fontsize=14, fontweight='bold')\n",
        "ax1.legend(fontsize=11)\n",
        "ax1.grid(True, alpha=0.3)\n",
        "\n",
        "ax2.plot(epochs, train_accs, 'b-', label='Train Accuracy', linewidth=2)\n",
        "ax2.plot(epochs, test_accs, 'r-', label='Test Accuracy', linewidth=2)\n",
        "ax2.set_xlabel('Epoch', fontsize=12)\n",
        "ax2.set_ylabel('Accuracy (%)', fontsize=12)\n",
        "ax2.set_title('Training and Test Accuracy', fontsize=14, fontweight='bold')\n",
        "ax2.legend(fontsize=11)\n",
        "ax2.grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.savefig('training_curves_improved.png', dpi=150)\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "\n",
        "## Final Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "print(\"=\"*70)\n",
        "print(\"FINAL RESULTS - IMPROVED VERSION\")\n",
        "print(\"=\"*70)\n",
        "print(f\"Overall Classification Test Accuracy: {best_acc:.2f}%\")\n",
        "print(f\"\\nImprovement over base: {best_acc - 83.89:.2f}%\")\n",
        "print(\"=\"*70)\n",
        "print(f\"\\nModel: best_vit_improved.pth\")\n",
        "print(f\"Plots: training_curves_improved.png\")"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.8.0"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 4
}